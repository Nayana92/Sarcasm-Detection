{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from langdetect import detect  \n",
    "import ast\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, json\n",
    "from nltk import word_tokenize,pos_tag_sents,WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import nltk\n",
    "# from polyglot.detect import Detector \n",
    "# import cld2\n",
    "from sklearn.ensemble import GradientBoostingClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the emoticon look up table file\n",
    "header = ['EmoticonSymbol','SentimentScore']\n",
    "emoticon_data = pd.read_csv('EmoticonLookupTable.txt', delimiter='\\t', encoding = 'ISO-8859-1',names=header)\n",
    "#Writing emoticons to a dictionary\n",
    "emoji_dict = emoticon_data.groupby('EmoticonSymbol')['SentimentScore'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting acronyms and slangs from html page and creating a dictionary\n",
    "resp = requests.get(\"http://www.netlingo.com/acronyms.php\")\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "slangdict= {}\n",
    "key=\"\"\n",
    "value=\"\"\n",
    "for div in soup.findAll('div', attrs={'class':'list_box3'}):\n",
    "  for li in div.findAll('li'):\n",
    "   for a in li.findAll('a'):\n",
    "       key =a.text\n",
    "   value = li.text.split(key)[1]\n",
    "   slangdict[key]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the \"-or-\" terms in the dictionary and retaining one acronym\n",
    "for key,value in slangdict.items():\n",
    "    if \"-or-\" in value:\n",
    "       removestring = re.findall('-or-(.*)',value)\n",
    "       removestring = ''.join(removestring)\n",
    "       newvalue = value.replace(removestring,'')\n",
    "       newvalue = newvalue.replace(\"-or-\",'')\n",
    "       slangdict[key] = newvalue\n",
    "    elif \"-or\" in value:\n",
    "       removestring = re.findall('-or(.*)',value)\n",
    "       removestring = ''.join(removestring)\n",
    "       newvalue = value.replace(removestring,'')\n",
    "       newvalue = newvalue.replace(\"-or\",'')\n",
    "       slangdict[key] = newvalue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_be_replaced = []\n",
    "for keys in slangdict.keys():\n",
    "    if \" or \" in keys:\n",
    "        key_to_be_replaced.append(keys)\n",
    "\n",
    "#print(key_to_be_replaced)\n",
    "\n",
    "for keys in key_to_be_replaced:\n",
    "    getkeys = keys.split(\"or\")\n",
    "    for x in getkeys:\n",
    "        x = x.strip()\n",
    "        slangdict[x]= slangdict[keys]\n",
    "    slangdict.pop(keys,None)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Slangdictionary.txt\",'w',encoding='utf-8')\n",
    "file.write(str(slangdict))\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the training data from the csv file\n",
    "header = ['label','comment','parent_comment']\n",
    "cleaneddata = pd.read_table('clean_data_test_balanced_Wparent.csv',\n",
    "                    sep='|', \n",
    "                   # delimiter=',',\n",
    "                    usecols=[0,1,2],\n",
    "                    dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(243572, 3)\n"
     ]
    }
   ],
   "source": [
    "print(cleaneddata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the slang dictionary that is already created\n",
    "f = open(\"Slangdictionary.txt\",\"r\")\n",
    "res1=f.read()\n",
    "f.close()\n",
    "slangdict = ast.literal_eval(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to extract features\n",
    "def featureextraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to get list of emojis in a comment\n",
    "def find_emoji(text):\n",
    "    return list(x for x in text.split() if x in emoji_dict.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features for each comment \n",
    "# Punctuation Features and presence of sarcastic symbol\n",
    "def allfeatures(user_comment):\n",
    "    if '!' or '.' or '?' in user_comment:\n",
    "        Numofexclaimations = user_comment.count('!')\n",
    "        Numofdots = user_comment.count('.')\n",
    "        Numofquestionmarks = user_comment.count('?')\n",
    "    else:\n",
    "        Numofexclaimations = 0\n",
    "        Numofdots = 0\n",
    "        Numofquestionmarks = 0\n",
    "    if '(!)' in user_comment:\n",
    "        SarcasticSymbol = 1\n",
    "    else:\n",
    "        SarcasticSymbol = 0\n",
    "    sentiments = TextBlob(str(user_comment)).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    numofcapitals = sum(x.isupper() for x in user_comment.split() if len(x) > 1 )\n",
    "    elist = find_emoji(user_comment)\n",
    "    pscore =0\n",
    "    nscore = 0\n",
    "    for item in elist:\n",
    "        if (emoji_dict[item][0] == 1):\n",
    "            pscore += 1\n",
    "        elif (emoji_dict[item][0] == -1):\n",
    "            nscore += 1\n",
    "    return Numofexclaimations,Numofdots,Numofquestionmarks,SarcasticSymbol,polarity,subjectivity,numofcapitals,pscore,nscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  461.1776177883148\n"
     ]
    }
   ],
   "source": [
    "#feature set 1\n",
    "start_time = time.time() \n",
    "featureddataset = featureextraction(cleaneddata, 'comment', allfeatures, ['Numofexclaimations', 'Numofdots','Numofquestionmarks','SarcasticSymbol','Polarity', 'Subjectivity','NumofCapitalWords','PositiveEmojiCount','NegativeEmojiCount'])\n",
    "end_time = time.time() \n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to collect number of interjection\n",
    "def comment_interjection(user_comment):\n",
    "    count = Counter(tag for word,tag in user_comment)\n",
    "    return count['UH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  137.26815843582153\n"
     ]
    }
   ],
   "source": [
    "#feature 2\n",
    "#feature extraction using POS\n",
    "start_time = time.time()\n",
    "txt = cleaneddata['comment'].tolist()\n",
    "#POS tagging for all the tokens in the sentence\n",
    "tagged_texts = pos_tag_sents(map(word_tokenize, txt))\n",
    "end_time = time.time()\n",
    "cleaneddata['POS'] = tagged_texts\n",
    "print(\"time taken \", end_time-start_time)\n",
    "\n",
    "# number of interjection\n",
    "featureddataset['interjection']  = cleaneddata.POS.apply(comment_interjection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "sia = SIA()\n",
    "#feature set 3\n",
    "#get heighly emotional words (associated with POS tags)\n",
    "def get_high_emotion_words(postags):\n",
    "    highly_pos = 0\n",
    "    highly_neg = 0\n",
    "    POS_list = ['JJ','JJR','JJS', 'RB','RBR','RBS','VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "    num_tokens = len(postags)\n",
    "    for i in range(num_tokens):\n",
    "        if postags[i][1] in POS_list:            \n",
    "            #check sentiment of next word\n",
    "            if i < (num_tokens - 1) :\n",
    "                senti_word = sia.polarity_scores(postags[i+1][0])\n",
    "                if senti_word['pos'] == 1:\n",
    "                    highly_pos += 1\n",
    "                if senti_word['neg'] == 1:\n",
    "                    highly_neg += 1\n",
    "                \n",
    "    return highly_pos, highly_neg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureddataset['highly_positive'],featureddataset['highly_negative'] = zip(*cleaneddata['POS'].map(get_high_emotion_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_parent_sentiment(comment):\n",
    "    sentiments = TextBlob(str(comment)).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    if polarity >= 0.1:\n",
    "        return 1\n",
    "    elif polarity < -0.1:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  119.58494448661804\n"
     ]
    }
   ],
   "source": [
    "# feature set 4\n",
    "start_time = time.time()\n",
    "featureddataset['parent_sentiment'] = cleaneddata.parent_comment.apply(get_parent_sentiment)\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional features pos_words, neg_words, flip_count\n",
    "def get_pos_neg_word_count(tokens):\n",
    "    \n",
    "    pos_word_count = 0\n",
    "    neg_word_count = 0    \n",
    "    pos_flag = False\n",
    "    neg_flag = False\n",
    "    flip_count = 0\n",
    "        \n",
    "    for word in tokens:\n",
    "        senti = sia.polarity_scores(str(word)) \n",
    "        if senti[\"pos\"] == 1:\n",
    "            pos_word_count += 1\n",
    "            pos_flag = True\n",
    "            if neg_flag:\n",
    "                flip_count += 1\n",
    "                neg_flag =  False\n",
    "                \n",
    "        elif senti[\"neg\"] == 1:\n",
    "            neg_word_count += 1\n",
    "            neg_flag = True\n",
    "            if pos_flag:\n",
    "                flip_count +=1\n",
    "                pos_flag = False\n",
    "    return pos_word_count, neg_word_count,flip_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  191.66995072364807\n"
     ]
    }
   ],
   "source": [
    "# feature set 5\n",
    "start_time = time.time()\n",
    "emotion_dataset = featureextraction(featureddataset, 'comment', get_pos_neg_word_count, ['PosWords','NegWords','FlipCount'])\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureddataset = featureextraction(featureddataset, 'comment', get_pos_neg_word_count, ['PosWords','NegWords','FlipCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# \n",
    "# load the Glove embedding into memory\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# Tokenize the comments\n",
    "Word_tokenizer = Tokenizer()\n",
    "Word_tokenizer.fit_on_texts(featureddataset['comment'])\n",
    "# Word_tokenizer.num_words = 100000\n",
    "vocab_size = len(Word_tokenizer.word_index) + 1\n",
    "#encode the train tokens to sequence\n",
    "sequences = Word_tokenizer.texts_to_sequences(featureddataset['comment'])\n",
    "\n",
    "# create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in Word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featureddataset['embedding'] = sequences\n",
    "emotion_dataset['embedding'] = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# # Tokenize the comments\n",
    "# Word_tokenizer = Tokenizer()\n",
    "# Word_tokenizer.fit_on_texts(featureddataset['comment'])\n",
    "# # Word_tokenizer.num_words = 100000\n",
    "# vocab_size = len(Word_tokenizer.word_index) + 1\n",
    "# #encode the train tokens to sequence\n",
    "# sequences = Word_tokenizer.texts_to_sequences(featureddataset['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = np.zeros((vocab_size, 100))\n",
    "# for word, i in Word_tokenizer.word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79339, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#helper to calculate the squareroot of square of the embedding matrix\n",
    "embedding_square ={}\n",
    "keys = range(vocab_size)\n",
    "for i in keys:\n",
    "    embedding = embedding_matrix[i]\n",
    "    sum_square = 0\n",
    "    for j in range(len(embedding)):\n",
    "        values = embedding[j]\n",
    "        sum_square += values*values\n",
    "    embedding_square[i] = math.sqrt(sum_square)\n",
    "# print(embedding_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79339"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"embedding_square_test.txt\",'w',encoding='utf-8')\n",
    "file.write(str(embedding_square))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# helper to calculate the cosine similarity between two words\n",
    "def cosine_similarity(word1,word2,v1,v2):\n",
    "#     \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxy = 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxy += x*y\n",
    "    return sumxy/(embedding_square[word1]*embedding_square[word2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to calculate the cosine similarity between the words in te comments\n",
    "def calculate_similarity(comment_token):\n",
    "    token_array = np.matrix(comment_token)\n",
    "    comment_len = token_array.shape[1]\n",
    "    most_similar = least_similar = most_dissimilar = least_dissimilar = 0\n",
    "    if comment_len > 0:\n",
    "        mat = np.empty(shape=(comment_len,comment_len))\n",
    "        mat[:] = np.nan\n",
    "        for i in range(0,comment_len):\n",
    "            for j in range(i+1,comment_len):\n",
    "                a = comment_token[i]\n",
    "                b = comment_token[j]\n",
    "                mat[i][j] = cosine_similarity(a,b,embedding_matrix[a],embedding_matrix[b].T)\n",
    "                mat[j][i] = mat[i][j]\n",
    "    #get the most similar\n",
    "        similar_mat = np.nanmax(mat,axis=0)\n",
    "        most_similar = np.nanmax(similar_mat)\n",
    "        least_similar = np.nanmin(similar_mat)\n",
    "    #get the most dissimilar \n",
    "        dissimilar_mat = np.nanmin(mat,axis=0)\n",
    "        most_dissimilar = np.nanmax(dissimilar_mat)\n",
    "        least_dissimilar = np.nanmin(dissimilar_mat)\n",
    "        \n",
    "    return most_similar, least_similar, most_dissimilar, least_dissimilar            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: All-NaN slice encountered\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: All-NaN slice encountered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  1624.5822041034698\n"
     ]
    }
   ],
   "source": [
    "# feature set 6\n",
    "start_time = time.time()\n",
    "embedded_dataset = featureextraction(emotion_dataset, 'embedding', calculate_similarity, ['most_similar','least_similar','most_dissimilar','least_dissimilar'])\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {'most_similar': 0, 'least_similar': 0, 'most_dissimilar': 0, 'least_dissimilar': 0}\n",
    "embedded_dataset = embedded_dataset.fillna(value=values)\n",
    "# embedded_dataset = embedded_dataset.drop(columns=['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>Numofexclaimations</th>\n",
       "      <th>Numofdots</th>\n",
       "      <th>Numofquestionmarks</th>\n",
       "      <th>SarcasticSymbol</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>NumofCapitalWords</th>\n",
       "      <th>...</th>\n",
       "      <th>highly_negative</th>\n",
       "      <th>parent_sentiment</th>\n",
       "      <th>PosWords</th>\n",
       "      <th>NegWords</th>\n",
       "      <th>FlipCount</th>\n",
       "      <th>embedding</th>\n",
       "      <th>most_similar</th>\n",
       "      <th>least_similar</th>\n",
       "      <th>most_dissimilar</th>\n",
       "      <th>least_dissimilar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Actually most of her supporters and sane peopl...</td>\n",
       "      <td>Hillary's Surrogotes Told to Blame Media for '...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[135, 160, 8, 112, 1388, 5, 3768, 40, 497, 32,...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.126212</td>\n",
       "      <td>0.068733</td>\n",
       "      <td>-0.133883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>They can't survive without an echo chamber whi...</td>\n",
       "      <td>Thank God Liberals like to live in concentrate...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[20, 89, 2783, 244, 52, 4613, 6090, 168, 7, 13...</td>\n",
       "      <td>0.812221</td>\n",
       "      <td>0.351907</td>\n",
       "      <td>0.307810</td>\n",
       "      <td>0.149742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>you're pretty cute yourself 1729 total</td>\n",
       "      <td>Saw this cutie training his Attack today...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[75, 134, 1292, 460, 37933, 1260]</td>\n",
       "      <td>0.639487</td>\n",
       "      <td>-0.068457</td>\n",
       "      <td>-0.068457</td>\n",
       "      <td>-0.162475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>If you kill me you'll crash the meme market</td>\n",
       "      <td>If you were locked in a room with 49 other peo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[27, 6, 328, 51, 551, 2285, 1, 1224, 622]</td>\n",
       "      <td>0.885117</td>\n",
       "      <td>0.045072</td>\n",
       "      <td>0.045072</td>\n",
       "      <td>-0.168710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I bet he wrote that last message as he was sob...</td>\n",
       "      <td>You're not even that pretty!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[4, 404, 28, 1516, 10, 216, 727, 31, 28, 25, 2...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.206977</td>\n",
       "      <td>0.206977</td>\n",
       "      <td>-0.079646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  Actually most of her supporters and sane peopl...   \n",
       "1      0  They can't survive without an echo chamber whi...   \n",
       "2      0             you're pretty cute yourself 1729 total   \n",
       "3      0        If you kill me you'll crash the meme market   \n",
       "4      0  I bet he wrote that last message as he was sob...   \n",
       "\n",
       "                                      parent_comment  Numofexclaimations  \\\n",
       "0  Hillary's Surrogotes Told to Blame Media for '...                 0.0   \n",
       "1  Thank God Liberals like to live in concentrate...                 0.0   \n",
       "2        Saw this cutie training his Attack today...                 0.0   \n",
       "3  If you were locked in a room with 49 other peo...                 0.0   \n",
       "4                       You're not even that pretty!                 0.0   \n",
       "\n",
       "   Numofdots  Numofquestionmarks  SarcasticSymbol  Polarity  Subjectivity  \\\n",
       "0        5.0                 0.0              0.0      0.50      0.500000   \n",
       "1        1.0                 0.0              0.0      0.80      0.750000   \n",
       "2        0.0                 0.0              0.0      0.25      0.916667   \n",
       "3        0.0                 0.0              0.0      0.00      0.000000   \n",
       "4        1.0                 0.0              0.0      0.00      0.066667   \n",
       "\n",
       "   NumofCapitalWords       ...         highly_negative  parent_sentiment  \\\n",
       "0                0.0       ...                       0                 0   \n",
       "1                0.0       ...                       0                 0   \n",
       "2                0.0       ...                       0                 0   \n",
       "3                0.0       ...                       0                -1   \n",
       "4                0.0       ...                       1                 1   \n",
       "\n",
       "   PosWords  NegWords  FlipCount  \\\n",
       "0         0         0          0   \n",
       "1         0         0          0   \n",
       "2         0         0          0   \n",
       "3         0         0          0   \n",
       "4         0         0          0   \n",
       "\n",
       "                                           embedding  most_similar  \\\n",
       "0  [135, 160, 8, 112, 1388, 5, 3768, 40, 497, 32,...      1.000000   \n",
       "1  [20, 89, 2783, 244, 52, 4613, 6090, 168, 7, 13...      0.812221   \n",
       "2                  [75, 134, 1292, 460, 37933, 1260]      0.639487   \n",
       "3          [27, 6, 328, 51, 551, 2285, 1, 1224, 622]      0.885117   \n",
       "4  [4, 404, 28, 1516, 10, 216, 727, 31, 28, 25, 2...      1.000000   \n",
       "\n",
       "   least_similar  most_dissimilar least_dissimilar  \n",
       "0       0.126212         0.068733        -0.133883  \n",
       "1       0.351907         0.307810         0.149742  \n",
       "2      -0.068457        -0.068457        -0.162475  \n",
       "3       0.045072         0.045072        -0.168710  \n",
       "4       0.206977         0.206977        -0.079646  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_dataset.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the cleaned data with features into a csv file\n",
    "embedded_dataset.to_csv('clean_testdata_with_all_features.csv',\n",
    "           sep= '|',\n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # embedded_dataset.drop(columns=['embedding'])\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # training_data, test_data = train_test_split(embedded_dataset, test_size=0.20)\n",
    "# training_data, test_data, y_train, y_test = train_test_split(embedded_dataset, embedded_dataset['label'], \n",
    "#                                                     test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train data with features\n",
    "traindata_withfeature = pd.read_table('clean_data_with_all_features.csv',\n",
    "                    sep='|', \n",
    "                   # delimiter=',',\n",
    "#                     usecols=[0,1,2],\n",
    "                    dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)\n",
    "\n",
    "# load the test data with features\n",
    "testdata_withfeature = pd.read_table('clean_testdata_with_all_features.csv',\n",
    "                    sep='|', \n",
    "                   # delimiter=',',\n",
    "#                     usecols=[0,1,2],\n",
    "                    dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>Numofexclaimations</th>\n",
       "      <th>Numofdots</th>\n",
       "      <th>Numofquestionmarks</th>\n",
       "      <th>SarcasticSymbol</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>NumofCapitalWords</th>\n",
       "      <th>...</th>\n",
       "      <th>highly_negative</th>\n",
       "      <th>parent_sentiment</th>\n",
       "      <th>PosWords</th>\n",
       "      <th>NegWords</th>\n",
       "      <th>FlipCount</th>\n",
       "      <th>embedding</th>\n",
       "      <th>most_similar</th>\n",
       "      <th>least_similar</th>\n",
       "      <th>most_dissimilar</th>\n",
       "      <th>least_dissimilar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Actually most of her supporters and sane peopl...</td>\n",
       "      <td>Hillary's Surrogotes Told to Blame Media for '...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[135, 160, 8, 112, 1388, 5, 3768, 40, 497, 32,...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.126212</td>\n",
       "      <td>0.068733</td>\n",
       "      <td>-0.133883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>They can't survive without an echo chamber whi...</td>\n",
       "      <td>Thank God Liberals like to live in concentrate...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[20, 89, 2783, 244, 52, 4613, 6090, 168, 7, 13...</td>\n",
       "      <td>0.812221</td>\n",
       "      <td>0.351907</td>\n",
       "      <td>0.307810</td>\n",
       "      <td>0.149742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>you're pretty cute yourself 1729 total</td>\n",
       "      <td>Saw this cutie training his Attack today...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[75, 134, 1292, 460, 37933, 1260]</td>\n",
       "      <td>0.639487</td>\n",
       "      <td>-0.068457</td>\n",
       "      <td>-0.068457</td>\n",
       "      <td>-0.162475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>If you kill me you'll crash the meme market</td>\n",
       "      <td>If you were locked in a room with 49 other peo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[27, 6, 328, 51, 551, 2285, 1, 1224, 622]</td>\n",
       "      <td>0.885117</td>\n",
       "      <td>0.045072</td>\n",
       "      <td>0.045072</td>\n",
       "      <td>-0.168710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I bet he wrote that last message as he was sob...</td>\n",
       "      <td>You're not even that pretty!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[4, 404, 28, 1516, 10, 216, 727, 31, 28, 25, 2...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.206977</td>\n",
       "      <td>0.206977</td>\n",
       "      <td>-0.079646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  Actually most of her supporters and sane peopl...   \n",
       "1      0  They can't survive without an echo chamber whi...   \n",
       "2      0             you're pretty cute yourself 1729 total   \n",
       "3      0        If you kill me you'll crash the meme market   \n",
       "4      0  I bet he wrote that last message as he was sob...   \n",
       "\n",
       "                                      parent_comment  Numofexclaimations  \\\n",
       "0  Hillary's Surrogotes Told to Blame Media for '...                 0.0   \n",
       "1  Thank God Liberals like to live in concentrate...                 0.0   \n",
       "2        Saw this cutie training his Attack today...                 0.0   \n",
       "3  If you were locked in a room with 49 other peo...                 0.0   \n",
       "4                       You're not even that pretty!                 0.0   \n",
       "\n",
       "   Numofdots  Numofquestionmarks  SarcasticSymbol  Polarity  Subjectivity  \\\n",
       "0        5.0                 0.0              0.0      0.50      0.500000   \n",
       "1        1.0                 0.0              0.0      0.80      0.750000   \n",
       "2        0.0                 0.0              0.0      0.25      0.916667   \n",
       "3        0.0                 0.0              0.0      0.00      0.000000   \n",
       "4        1.0                 0.0              0.0      0.00      0.066667   \n",
       "\n",
       "   NumofCapitalWords       ...         highly_negative  parent_sentiment  \\\n",
       "0                0.0       ...                       0                 0   \n",
       "1                0.0       ...                       0                 0   \n",
       "2                0.0       ...                       0                 0   \n",
       "3                0.0       ...                       0                -1   \n",
       "4                0.0       ...                       1                 1   \n",
       "\n",
       "   PosWords  NegWords  FlipCount  \\\n",
       "0         0         0          0   \n",
       "1         0         0          0   \n",
       "2         0         0          0   \n",
       "3         0         0          0   \n",
       "4         0         0          0   \n",
       "\n",
       "                                           embedding  most_similar  \\\n",
       "0  [135, 160, 8, 112, 1388, 5, 3768, 40, 497, 32,...      1.000000   \n",
       "1  [20, 89, 2783, 244, 52, 4613, 6090, 168, 7, 13...      0.812221   \n",
       "2                  [75, 134, 1292, 460, 37933, 1260]      0.639487   \n",
       "3          [27, 6, 328, 51, 551, 2285, 1, 1224, 622]      0.885117   \n",
       "4  [4, 404, 28, 1516, 10, 216, 727, 31, 28, 25, 2...      1.000000   \n",
       "\n",
       "   least_similar  most_dissimilar least_dissimilar  \n",
       "0       0.126212         0.068733        -0.133883  \n",
       "1       0.351907         0.307810         0.149742  \n",
       "2      -0.068457        -0.068457        -0.162475  \n",
       "3       0.045072         0.045072        -0.168710  \n",
       "4       0.206977         0.206977        -0.079646  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata_withfeature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop embedding\n",
    "traindata_withfeature = traindata_withfeature.drop(columns=['embedding'])\n",
    "\n",
    "testdata_withfeature = testdata_withfeature.drop(columns=['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop parent sentiment\n",
    "# parent_sentiment\n",
    "traindata_Wparent = traindata_withfeature\n",
    "traindata_Wparent = traindata_Wparent.drop(columns=['parent_sentiment'])\n",
    "\n",
    "testdata_Wparent = testdata_withfeature\n",
    "testdata_Wparent = testdata_Wparent.drop(columns=['parent_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop new added features(5)\n",
    "traindata_parent = traindata_withfeature\n",
    "traindata_parent = traindata_parent.drop(columns=['highly_positive','highly_negative','PosWords','NegWords','FlipCount'])\n",
    "\n",
    "testdata_parent = testdata_withfeature\n",
    "testdata_parent = testdata_parent.drop(columns=['highly_positive','highly_negative','PosWords','NegWords','FlipCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with only embeddings\n",
    "traindata = traindata_withfeature\n",
    "traindata = traindata.drop(columns=['parent_sentiment','highly_positive','highly_negative','PosWords','NegWords','FlipCount'])\n",
    "\n",
    "testdata = testdata_withfeature\n",
    "testdata = testdata.drop(columns=['parent_sentiment','highly_positive','highly_negative','PosWords','NegWords','FlipCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(978039, 17)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata.shape\n",
    "traindata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data features\n",
    "newtrain = pd.DataFrame(traindata_parent.iloc[:, 3:])\n",
    "#train data labels\n",
    "targetlabel = traindata_parent.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data features\n",
    "newtest = pd.DataFrame(testdata_parent.iloc[:, 3:])\n",
    "#test data labels\n",
    "testlabel = testdata_parent.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  52.24272012710571\n"
     ]
    }
   ],
   "source": [
    "# gradient boosting algorithm\n",
    "start_time = time.time()\n",
    "gradient_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5, max_depth=1)\n",
    "gradient_clf.fit(newtrain,targetlabel)\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_predictions = gradient_clf.predict(newtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[78483 42081]\n",
      " [54489 68519]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.65      0.62    120564\n",
      "           1       0.62      0.56      0.59    123008\n",
      "\n",
      "   micro avg       0.60      0.60      0.60    243572\n",
      "   macro avg       0.60      0.60      0.60    243572\n",
      "weighted avg       0.61      0.60      0.60    243572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(testlabel, gradient_predictions))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(testlabel, gradient_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  23.141639947891235\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "start_time = time.time()\n",
    "random_clf = RandomForestClassifier(n_jobs=2, random_state=0)\n",
    "random_clf.fit(newtrain,targetlabel)\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_predictions = random_clf.predict(newtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[80770 39794]\n",
      " [60120 62888]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.67      0.62    120564\n",
      "           1       0.61      0.51      0.56    123008\n",
      "\n",
      "   micro avg       0.59      0.59      0.59    243572\n",
      "   macro avg       0.59      0.59      0.59    243572\n",
      "weighted avg       0.59      0.59      0.59    243572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(testlabel, random_predictions))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(testlabel, random_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5897968567815677\n"
     ]
    }
   ],
   "source": [
    "# embedded_dataset.drop(columns=['embedding'])\n",
    "print(accuracy_score(testlabel, random_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtrain = pd.DataFrame(embedded_dataset.iloc[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(978039, 20)\n"
     ]
    }
   ],
   "source": [
    "print(newtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetlabel = embedded_dataset.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1010781,)\n"
     ]
    }
   ],
   "source": [
    "print(targetlabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with with parent comment concatinated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  21.321204900741577\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# data['comment'] = data.comment.apply(comment_clean)\n",
    "data_test[['comment','parent_comment']] = data_test[['comment','parent_comment']].applymap(comment_clean)\n",
    "# remove data with empty comments\n",
    "valid_test_comment = data_test['comment'] != ' '\n",
    "data_test = data_test[valid_comment]\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "featureddataset = featureextraction(data_test, 'comment', allfeatures, ['Numofexclaimations', 'Numofdots','Numofquestionmarks','SarcasticSymbol','Polarity', 'Subjectivity','NumofCapitalWords','PositiveEmojiCount','NegativeEmojiCount'])\n",
    "end_time = time.time() \n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_new = test_data.iloc[:, 3:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202157, 15)\n"
     ]
    }
   ],
   "source": [
    "print(test_data_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_target = test_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202157,)\n"
     ]
    }
   ],
   "source": [
    "print(test_data_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(test_data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[71179 29832]\n",
      " [52228 48918]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.70      0.63    101011\n",
      "           1       0.62      0.48      0.54    101146\n",
      "\n",
      "   micro avg       0.59      0.59      0.59    202157\n",
      "   macro avg       0.60      0.59      0.59    202157\n",
      "weighted avg       0.60      0.59      0.59    202157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(test_data_target, predictions))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(test_data_target, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5940778701702142\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_data_target, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "# for learning_rate in learning_rates:\n",
    "#     gb = GradientBoostingClassifier(n_estimators=20, learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0)\n",
    "#     gb.fit(newtrain,targetlabel)\n",
    "#     predictions = clf.predict(test_data_new)\n",
    "#     print(\"Confusion Matrix\")\n",
    "#     print(confusion_matrix(test_data_target, predictions))\n",
    "#     print()\n",
    "#     print(\"Classification Report\")\n",
    "#     print(classification_report(test_data_target, predictions))\n",
    "#     print()\n",
    "#     print(accuracy_score(test_data_target, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the feature column's names\n",
    "features = embedded_dataset.columns[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data[features]\n",
    "y_train = training_data['label']\n",
    "X_test = test_data[features]\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create a random forest Classifier. \n",
    "random_clf = RandomForestClassifier(n_jobs=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  14.977846622467041\n"
     ]
    }
   ],
   "source": [
    "# Train the Classifier to take the training features \n",
    "start_time = time.time()\n",
    "random_clf.fit(X_train, y_train )\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Classifier we trained to the test data\n",
    "y_predit = random_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[69374 31637]\n",
      " [51588 49558]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.69      0.63    101011\n",
      "           1       0.61      0.49      0.54    101146\n",
      "\n",
      "   micro avg       0.59      0.59      0.59    202157\n",
      "   macro avg       0.59      0.59      0.58    202157\n",
      "weighted avg       0.59      0.59      0.58    202157\n",
      "\n",
      "0.588315022482526\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_predit))  \n",
    "print(classification_report(y_test,y_predit))  \n",
    "print(accuracy_score(y_test,y_predit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing on the test dataset\n",
    "#read the testing data from the csv file\n",
    "#read the training data from the csv file\n",
    "header = ['label','comment','parent_comment']\n",
    "test_clean = pd.read_table('clean_data_test_balanced_final.csv',\n",
    "                    sep='|', \n",
    "                    usecols=[0,1,2],\n",
    "                    dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)\n",
    "\n",
    "#extract features\n",
    "start_time = time.time() \n",
    "feature_testdata = featureextraction(test_clean, 'comment', allfeatures, ['Numofexclaimations', 'Numofdots','Numofquestionmarks','SarcasticSymbol','Polarity', 'Subjectivity','NumofCapitalWords','PositiveEmojiCount','NegativeEmojiCount'])\n",
    "end_time = time.time() \n",
    "\n",
    "txt = test_clean['comment'].tolist()\n",
    "#POS tagging for all the tokens in the sentence\n",
    "tagged_texts = pos_tag_sents(map(word_tokenize, txt))\n",
    "end_time = time.time()\n",
    "test_clean['POS'] = tagged_texts\n",
    "\n",
    "# number of interjection\n",
    "feature_testdata['interjection']  = test_clean.POS.apply(comment_interjection)\n",
    "\n",
    "#emotion intensity\n",
    "feature_testdata['highly_positive'],feature_testdata['highly_negative'] = zip(*cleaneddata['POS'].map(get_high_emotion_words))\n",
    "\n",
    "#parent comment\n",
    "feature_testdata['parent_sentiment'] = test_clean.parent_comment.apply(get_parent_sentiment)\n",
    "\n",
    "#positive and negative word count\n",
    "emotion_test_dataset = featureextraction(feature_testdata, 'comment', get_pos_neg_word_count, ['PosWords','NegWords','FlipCount'])\n",
    "\n",
    "#embedding matrix creation\n",
    "# Tokenize the comments\n",
    "Word_tokenizer_test = Tokenizer()\n",
    "Word_tokenizer_test.fit_on_texts(feature_testdata['comment'])\n",
    "# Word_tokenizer.num_words = 100000\n",
    "vocab_size = len(Word_tokenizer_test.word_index) + 1\n",
    "#encode the train tokens to sequence\n",
    "sequences = Word_tokenizer_test.texts_to_sequences(feature_testdata['comment'])\n",
    "\n",
    "# create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in Word_tokenizer_test.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "#add the embedding to dataset\n",
    "emotion_test_dataset['embedding'] = sequences\n",
    "\n",
    "# import math\n",
    "#helper to calculate the squareroot of square of the embedding matrix\n",
    "embedding_square ={}\n",
    "keys = range(vocab_size)\n",
    "for i in keys:\n",
    "    embedding = embedding_matrix[i]\n",
    "    sum_square = 0\n",
    "    for j in range(len(embedding)):\n",
    "        values = embedding[j]\n",
    "        sum_square += values*values\n",
    "    embedding_square[i] = math.sqrt(sum_square)\n",
    "\n",
    "#similarity features\n",
    "embedded_test_dataset = featureextraction(emotion_test_dataset, 'embedding', calculate_similarity, ['most_similar','least_similar','most_dissimilar','least_dissimilar'])\n",
    "print(\"time taken \", end_time-start_time)\n",
    "\n",
    "values = {'most_similar': 0, 'least_similar': 0, 'most_dissimilar': 0, 'least_dissimilar': 0}\n",
    "embedded_test_dataset = embedded_test_dataset.fillna(value=values)\n",
    "embedded_test_dataset = embedded_test_dataset.drop(columns=['embedding'])\n",
    "\n",
    "#split the test data into the comment and label\n",
    "test_data_new = test_data.iloc[:, 3:] \n",
    "test_data_target = test_data.iloc[:,0]\n",
    "#predict using gradient boosting\n",
    "predictions_gradientboost = gradient_clf.predict(test_data_new)\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(test_data_target, predictions_gradientboost))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(test_data_target, predictions_gradientboost))\n",
    "print(accuracy_score(test_data_target, predictions_gradientboost))\n",
    "\n",
    "#predict using random forest\n",
    "predictions_randomforest = random_clf.predict(test_data_new)\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(test_data_target, predictions_randomforest))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(test_data_target, predictions_randomforest))\n",
    "print(accuracy_score(test_data_target, predictions_randomforest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "start_time = time.time()\n",
    "# linear kernel\n",
    "svclassifier = SVC(kernel='linear',C=1,gamma=1)  \n",
    "svclassifier.fit(X_train, y_train) \n",
    "\n",
    "y_pred = svclassifier.predict(X_test)  \n",
    "\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guassian kernel\n",
    "start_time = time.time()\n",
    "svclassifier = SVC(kernel='rbf',C=1,gamma=1)  \n",
    "svclassifier.fit(X_train, y_train) \n",
    "\n",
    "y_pred = svclassifier.predict(X_test)  \n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial kernel with degree 3\n",
    "start_time = time.time()\n",
    "svclassifier = SVC(kernel='poly', degree=3) \n",
    "svclassifier.fit(X_train, y_train) \n",
    "\n",
    "y_pred = svclassifier.predict(X_test) \n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
