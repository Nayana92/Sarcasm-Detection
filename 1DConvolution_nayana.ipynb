{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " '1DConvolution_nayana.ipynb',\n",
       " 'Classification_RF.ipynb',\n",
       " 'clean_data.csv',\n",
       " 'clean_data.ipynb',\n",
       " 'clean_data_cld2withpoly.csv',\n",
       " 'clean_data_Cldpoly.csv',\n",
       " 'clean_data_test_balanced_final.csv',\n",
       " 'clean_data_train_balanced_final.csv',\n",
       " 'clean_data_with features.csv',\n",
       " 'clean_data_with_all_features.csv',\n",
       " 'clean_data_with_all_features.zip',\n",
       " 'embedding_square.txt',\n",
       " 'embedding_square_train.txt',\n",
       " 'Emoticon and Slang dictionary Creation.ipynb',\n",
       " 'EmoticonLookupTable.txt',\n",
       " 'Featuredataset.csv',\n",
       " 'glove.6B.100d.txt',\n",
       " 'Handcrafted_features.finalML.ipynb',\n",
       " 'neural_net with parent comment.html',\n",
       " 'neural_net with parent comment_files',\n",
       " 'preprocessed_added_features.ipynb',\n",
       " 'preprocessed_added_features_merged.ipynb',\n",
       " 'preprocessed_features_classifiers.ipynb',\n",
       " 'preprocessed_features_classifiers_withembedding.ipynb',\n",
       " 'README.md',\n",
       " 'sarcasmdetection_RF.ipynb',\n",
       " 'Slangdictionary.txt',\n",
       " 'test-balanced.csv',\n",
       " 'text_process.ipynb',\n",
       " 'train-balanced.csv',\n",
       " 'Untitled.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get relative path\n",
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['label','comment','parent_comment']\n",
    "train_clean = pd.read_table('clean_data_train_balanced_final.csv',\n",
    "                    sep='|', \n",
    "                    usecols=[0,1,2],\n",
    "                    dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = pd.read_table('clean_data_test_balanced_final.csv',\n",
    "                    sep='|', \n",
    "                    usecols=[0,1],\n",
    "                    dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_comments = train_clean['comment']\n",
    "train_labels = train_clean['label']\n",
    "SEED = 100\n",
    "\n",
    "test_comments = test_clean['comment']\n",
    "test_labels = test_clean['label']\n",
    "\n",
    "#split the data into test and train\n",
    "# comments_train, comments_test, labels_train, labels_test = train_test_split(comments, labels, test_size=.20, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 978039 entries with 49.49% non-sarcastic, 50.51% sarcastic\n",
      "Test set has total 243572 entries with 49.50% non-sarcastic, 50.50% sarcastic\n"
     ]
    }
   ],
   "source": [
    "# print the number of sarcastic and non sarcastic comments in training set\n",
    "print (\"Train set has total {0} entries with {1:.2f}% non-sarcastic, {2:.2f}% sarcastic\".format(len(train_comments),\n",
    "                                (len(train_comments[train_labels == 0]) / (len(train_comments)*1.))*100,\n",
    "                                (len(train_comments[train_labels == 1]) / (len(train_comments)*1.))*100))\n",
    "# print the number of sarcastic and non sarcastic comments in testing set\n",
    "print (\"Test set has total {0} entries with {1:.2f}% non-sarcastic, {2:.2f}% sarcastic\".format(len(test_comments),\n",
    "                                (len(test_comments[test_labels == 0]) / (len(test_comments)*1.))*100,\n",
    "                                (len(test_comments[test_labels == 1]) / (len(test_comments)*1.))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1221611,)\n",
      "(978039,)\n",
      "(243572,)\n"
     ]
    }
   ],
   "source": [
    "frames = [train_comments, test_comments]\n",
    "mearged_comment = pd.concat(frames)\n",
    "print(mearged_comment.shape)\n",
    "print(train_comments.shape)\n",
    "print(test_comments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2222"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find maximum sizde of the token\n",
    "length = []\n",
    "for comment in comments:\n",
    "    length.append(len(comment.split()))\n",
    "max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize the train comments\n",
    "Word_tokenizer = Tokenizer()\n",
    "Word_tokenizer.fit_on_texts(mearged_comment)\n",
    "vocab_size = len(Word_tokenizer.word_index) + 1\n",
    "# Word_tokenizer.num_words = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (978039, 2250)\n"
     ]
    }
   ],
   "source": [
    "# process tokens in training set\n",
    "#encode the train tokens to sequence\n",
    "sequences_train = Word_tokenizer.texts_to_sequences(train_comments)\n",
    "#pading the sequence\n",
    "x_train_seq = pad_sequences(sequences_train, maxlen=2250)\n",
    "print('Shape of data tensor:', x_train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (243572, 2250)\n"
     ]
    }
   ],
   "source": [
    "# process tokens in testing set\n",
    "#encode the test tokens to sequence\n",
    "sequences_test = Word_tokenizer.texts_to_sequences(test_comments)\n",
    "#pading the sequence\n",
    "x_test_seq = pad_sequences(sequences_test, maxlen=2250)\n",
    "print('Shape of data tensor:', x_test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the Glove embedding into memory\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in train comments\n",
    "# consider maximum of 100000 from the vocabulary\n",
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in Word_tokenizer.word_index.items():\n",
    "#     if i >= num_words:\n",
    "#         continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188492, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in test comments\n",
    "# consider maximum of 100000 from the vocabulary\n",
    "num_words = 100000\n",
    "embedding_matrix_test = np.zeros((num_words, 100))\n",
    "for word, i in Word_tokenizer_test.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_test[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "seed = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 2250, 100)         18849200  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2250, 64)          19264     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 144000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 144000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               18432128  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 37,300,721\n",
      "Trainable params: 37,300,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model #1: Convolution neural network with 1 layer 1 embedding layer(trainable)\n",
    "# Activation layer : Sigmoid\n",
    "# loss : binary_crossentropy\n",
    "# optimizer : adam\n",
    "model_cnn_1 = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=2250, trainable=True)\n",
    "model_cnn_1.add(e)\n",
    "model_cnn_1.add(Conv1D(64, 3, padding='same'))\n",
    "model_cnn_1.add(Flatten())\n",
    "model_cnn_1.add(Dropout(0.2))\n",
    "model_cnn_1.add(Dense(128,activation='sigmoid'))\n",
    "model_cnn_1.add(Dropout(0.2))\n",
    "model_cnn_1.add(Dense(1,activation='sigmoid'))\n",
    "model_cnn_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(model_cnn_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath,monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 782431 samples, validate on 195608 samples\n",
      "Epoch 1/10\n",
      "782431/782431 [==============================] - 17851s 23ms/step - loss: 0.5736 - acc: 0.6982 - val_loss: 0.5528 - val_acc: 0.7150\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.71503, saving model to weights.best.hdf5\n",
      "Epoch 2/10\n",
      "782431/782431 [==============================] - 13380s 17ms/step - loss: 0.5394 - acc: 0.7267 - val_loss: 0.5521 - val_acc: 0.7157\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.71503 to 0.71571, saving model to weights.best.hdf5\n",
      "Epoch 3/10\n",
      "782431/782431 [==============================] - 13418s 17ms/step - loss: 0.5124 - acc: 0.7463 - val_loss: 0.5543 - val_acc: 0.7160\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.71571 to 0.71604, saving model to weights.best.hdf5\n",
      "Epoch 4/10\n",
      "782431/782431 [==============================] - 13260s 17ms/step - loss: 0.4875 - acc: 0.7632 - val_loss: 0.5647 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.71604\n",
      "Epoch 5/10\n",
      "782431/782431 [==============================] - 13222s 17ms/step - loss: 0.4671 - acc: 0.7745 - val_loss: 0.5834 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.71604\n",
      "Epoch 6/10\n",
      "782431/782431 [==============================] - 16152s 21ms/step - loss: 0.4503 - acc: 0.7836 - val_loss: 0.5899 - val_acc: 0.7096\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.71604\n",
      "Epoch 7/10\n",
      "782431/782431 [==============================] - 12981s 17ms/step - loss: 0.4348 - acc: 0.7915 - val_loss: 0.6102 - val_acc: 0.7048\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.71604\n",
      "Epoch 8/10\n",
      "782431/782431 [==============================] - 13067s 17ms/step - loss: 0.4213 - acc: 0.7982 - val_loss: 0.6244 - val_acc: 0.7006\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.71604\n",
      "Epoch 9/10\n",
      "782431/782431 [==============================] - 14134s 18ms/step - loss: 0.4099 - acc: 0.8030 - val_loss: 0.6430 - val_acc: 0.7020\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.71604\n",
      "Epoch 10/10\n",
      "782431/782431 [==============================] - 13355s 17ms/step - loss: 0.3981 - acc: 0.8092 - val_loss: 0.6590 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.71604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25bb3d70390>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model_cnn_1.fit(x_train_seq, train_labels, validation_split=0.2, epochs=10, batch_size=512, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#store the trained model.\n",
    "from keras.models import model_from_json\n",
    "#serialize model to JSON\n",
    "model_json = model_cnn_1.to_json()\n",
    "with open(\"model_cnn_1.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "#serialize weight to HDF4\n",
    "model_cnn_1.save_weights(\"model_cnn_1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#load json and create model\n",
    "json_file = open('model_cnn_1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_cnn_model = model_from_json(loaded_model_json)\n",
    "#load weights into new model\n",
    "loaded_cnn_model.load_weights(\"model_cnn_1.h5\")\n",
    "print(\"loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243572/243572 [==============================] - 1102s 5ms/step\n",
      "acc: 69.41%\n"
     ]
    }
   ],
   "source": [
    "# evaluate loadedmodel on test data\n",
    "loaded_cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_cnn_model.evaluate(x_test_seq,test_labels)\n",
    "print(\"%s: %.2f%%\" %(loaded_cnn_model.metrics_names[1],score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[91129 29435]\n",
      " [45076 77932]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.76      0.71    120564\n",
      "           1       0.73      0.63      0.68    123008\n",
      "\n",
      "   micro avg       0.69      0.69      0.69    243572\n",
      "   macro avg       0.70      0.69      0.69    243572\n",
      "weighted avg       0.70      0.69      0.69    243572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the confusion matrix\n",
    "predictions = loaded_cnn_model.predict_classes(x_test_seq)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(test_labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
