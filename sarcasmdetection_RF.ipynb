{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from langdetect import detect  \n",
    "import ast\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, json\n",
    "from nltk import word_tokenize,pos_tag_sents,WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from polyglot.detect import Detector \n",
    "import cld2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the training data from the csv file\n",
    "header = ['label','comment','parent_comment']\n",
    "data = pd.read_table('train-balanced.csv',\n",
    "                    sep='\\t', \n",
    "#                     delimiter=',', \n",
    "                    names=header,\n",
    "                    usecols=[0,1,9],\n",
    "                    dtype={'label':int,'comment':str, 'parent_comment':str},\n",
    "                    keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the emoticon look up table file\n",
    "header = ['EmoticonSymbol','SentimentScore']\n",
    "emoticon_data = pd.read_csv('EmoticonLookupTable.txt', delimiter='\\t', encoding = 'ISO-8859-1',names=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing emoticons to a dictionary\n",
    "emoji_dict = emoticon_data.groupby('EmoticonSymbol')['SentimentScore'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the slang dictionary that is already created\n",
    "f = open(\"Slangdictionary.txt\",\"r\")\n",
    "res1=f.read()\n",
    "f.close()\n",
    "slangdict = ast.literal_eval(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to clean the comments\n",
    "def comment_clean(user_comment):\n",
    "    # remove trailing \\r and \\n    \n",
    "    user_comment.rstrip('\\r\\n')\n",
    "    \n",
    "    #remove the # from hashtag\n",
    "    if '#' in user_comment:\n",
    "        hash_tag = re.search('#',user_comment)\n",
    "        if hash_tag is not None:\n",
    "            user_comment = user_comment.replace(hash_tag.group(0),' ')\n",
    "    #remove the redit tags(r/) from comment\n",
    "    if 'r/' in user_comment:\n",
    "        r_tag = re.search('r/',user_comment)\n",
    "        if r_tag is not None:\n",
    "            user_comment = user_comment.replace(r_tag.group(0),' ')\n",
    "    #remove the URL links from comments  \n",
    "    if 'HTTP' in user_comment:\n",
    "        # url of the form [link name](http://url)\n",
    "        url_link = re.search('\\[(.*)\\(HTTP(.*)\\)', user_comment)\n",
    "        if url_link is not None:\n",
    "            user_comment = user_comment.replace(url_link.group(0),' ')\n",
    "    if 'http' in user_comment:\n",
    "        # url of the form [link name](http://url)\n",
    "        url_link = re.search('\\[(.*)\\(http(.*)\\)', user_comment)\n",
    "        if url_link is not None:\n",
    "            user_comment = user_comment.replace(url_link.group(0),' ')\n",
    "        else:\n",
    "            #url of the form http:/\n",
    "            url_link = re.search('http(.*)', user_comment)\n",
    "            if url_link is not None:\n",
    "                user_comment = user_comment.replace(url_link.group(0),' ') \n",
    "                \n",
    "    # remove numbers from comments\n",
    "    user_comment_not_num = re.sub(r'\\d+', '', user_comment)   \n",
    "    \n",
    "    # Check if the comment has exactly 2 stars\n",
    "    if user_comment.count('*')==2:\n",
    "        boldwords = re.search(r\"\\*(.*?)\\*\",user_comment)\n",
    "        #print(boldwords.group(0))\n",
    "        # Check if the comments have any other text other than **\n",
    "        if boldwords.group(0) != \"**\":\n",
    "            Wordstocapitalize = re.findall(r\"\\*(.*?)\\*\",boldwords.group(0))\n",
    "            Wordstocapitalize = \"\".join( Wordstocapitalize)\n",
    "            # Replace the user comment with capitalized words\n",
    "            user_comment = user_comment.replace(boldwords.group(0),Wordstocapitalize.upper())\n",
    "    comment_words = re.sub(r\"[^a-zA-Z0-9\\s\\']\",\"\",user_comment)         \n",
    "    comment_words=comment_words.split()\n",
    "    for word in comment_words:\n",
    "        if word.upper() in slangdict.keys():\n",
    "            user_comment = user_comment.replace(word.upper(),slangdict[word.upper()])\n",
    "        elif word in slangdict.keys():\n",
    "            user_comment = user_comment.replace(word,slangdict[word]) \n",
    "        \n",
    "    # replace non english comments with empty string\n",
    "    try:\n",
    "        isReliable, textBytesFound, details = cld2.detect(user_comment_not_num)\n",
    "    except:\n",
    "        try_text = ''.join(x for x in user_comment_not_num if x.isprintable())\n",
    "        isReliable, textBytesFound, details = cld2.detect(try_text)\n",
    "    cld_match = details[0][0]\n",
    "    if not (cld_match == 'ENGLISH'):\n",
    "        poly_match = Detector(user_comment_not_num, quiet=True).language.name\n",
    "        if (poly_match != 'English'):\n",
    "            user_comment = ' '    \n",
    "#     text = ''.join([l for l in user_comment_not_num if unicodedata.category(unicode(l))[0] not in ('S', 'M', 'C')])\n",
    "#     isReliable, textBytesFound, details = cld2.detect(text)\n",
    "#     cld_match = details[0][0]\n",
    "#     if not (cld_match == 'ENGLISH'):\n",
    "#         poly_match = Detector(text, quiet=True).language.name\n",
    "#         if (poly_match != 'English'):\n",
    "#             user_comment = ' '               \n",
    "   \n",
    "            \n",
    "    return user_comment           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  87.768639087677\n"
     ]
    }
   ],
   "source": [
    "#clean each commentnon_eng_data = []\n",
    "start_time = time.time()\n",
    "# data['comment'] = data.comment.apply(comment_clean)\n",
    "data[['comment','parent_comment']] = data[['comment','parent_comment']].applymap(comment_clean)\n",
    "# remove data with empty comments\n",
    "valid_comment = data['comment'] != ' '\n",
    "data = data[valid_comment]\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['parent_comment'] = data.parent_comment.apply(comment_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the cleaned data into a csv file\n",
    "data.to_csv('clean_data_Cldpoly_withparent.csv',\n",
    "           sep= '|',\n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the training data from the csv file\n",
    "cleaneddata = pd.read_table('clean_data_Cldpoly_withparent.csv',\n",
    "                    sep='|', \n",
    "                   # delimiter=',',\n",
    "                    usecols=[0,1,2],\n",
    "                    dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Nice Crib  and Nice Hand.</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0                          Nice Crib  and Nice Hand.   \n",
       "1      0  You do know west teams play against west teams...   \n",
       "2      0  They were underdogs earlier today, but since G...   \n",
       "3      0  This meme isn't funny none of the \"new york ni...   \n",
       "4      0                    I could use one of those tools.   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaneddata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureextraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to get list of emojis in a comment\n",
    "def find_emoji(text):\n",
    "    return list(x for x in text.split() if x in emoji_dict.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features for each comment \n",
    "# Punctuation Features and presence of sarcastic symbol\n",
    "def allfeatures(user_comment):\n",
    "    if '!' or '.' or '?' in user_comment:\n",
    "        Numofexclaimations = user_comment.count('!')\n",
    "        Numofdots = user_comment.count('.')\n",
    "        Numofquestionmarks = user_comment.count('?')\n",
    "    else:\n",
    "        Numofexclaimations = 0\n",
    "        Numofdots = 0\n",
    "        Numofquestionmarks = 0\n",
    "    if '(!)' in user_comment:\n",
    "        SarcasticSymbol = 1\n",
    "    else:\n",
    "        SarcasticSymbol = 0\n",
    "    sentiments = TextBlob(str(user_comment)).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    numofcapitals = sum(x.isupper() for x in user_comment.split() if len(x) > 1 )\n",
    "    elist = find_emoji(user_comment)\n",
    "    pscore =0\n",
    "    nscore = 0\n",
    "    for item in elist:\n",
    "        if (emoji_dict[item][0] == 1):\n",
    "            pscore += 1\n",
    "        elif (emoji_dict[item][0] == -1):\n",
    "            nscore += 1\n",
    "    return Numofexclaimations,Numofdots,Numofquestionmarks,SarcasticSymbol,polarity,subjectivity,numofcapitals,pscore,nscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-906b1d060fdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatureddataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatureextraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaneddata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'comment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Numofexclaimations'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Numofdots'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Numofquestionmarks'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'SarcasticSymbol'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Polarity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Subjectivity'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'NumofCapitalWords'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'PositiveEmojiCount'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'NegativeEmojiCount'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-61-d606a7df9626>\u001b[0m in \u001b[0;36mfeatureextraction\u001b[1;34m(dataframe, field, func, column_names)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         dataframe[field].apply(\n\u001b[1;32m----> 5\u001b[1;33m             lambda cell: pd.Series(func(cell), index=column_names))), axis=1)\n\u001b[0m",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3192\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3194\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-d606a7df9626>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(cell)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         dataframe[field].apply(\n\u001b[1;32m----> 5\u001b[1;33m             lambda cell: pd.Series(func(cell), index=column_names))), axis=1)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-63-bb264c1811c7>\u001b[0m in \u001b[0;36mallfeatures\u001b[1;34m(user_comment)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mSarcasticSymbol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0msentiments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_comment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mpolarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0msubjectivity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubjectivity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\textblob\\decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36msentiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnamedtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mform\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubjectivity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         \"\"\"\n\u001b[1;32m--> 426\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\textblob\\en\\sentiments.py\u001b[0m in \u001b[0;36manalyze\u001b[1;34m(self, text, keep_assessments)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mSentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'polarity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'subjectivity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpattern_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36mnamedtuple\u001b[1;34m(typename, field_names, rename, defaults, module)\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[0mnamespace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'_tuple_new'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__name__'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mf'namedtuple_{typename}'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;31m# Note: exec() has the side-effect of interning the field names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m     \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__new__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[0m__new__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'Create new instance of {typename}({arg_list})'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "featureddataset = featureextraction(cleaneddata, 'comment', allfeatures, ['Numofexclaimations', 'Numofdots','Numofquestionmarks','SarcasticSymbol','Polarity', 'Subjectivity','NumofCapitalWords','PositiveEmojiCount','NegativeEmojiCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_sentiment(comment):\n",
    "    sentiments = TextBlob(str(comment)).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    if polarity >= 0.1:\n",
    "        return 1\n",
    "    elif polarity < -0.1:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "# sentiments = TextBlob(str(\"you are \")).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureddataset['parent_sentiment'] = cleaneddata.parent_comment.apply(get_parent_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  83.13779640197754\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "txt = cleaneddata['comment'].tolist()\n",
    "#POS tagging for all the tokens in the sentence\n",
    "tagged_texts = pos_tag_sents(map(word_tokenize, txt))\n",
    "end_time = time.time()\n",
    "cleaneddata['POS'] = tagged_texts\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#helper function to collect number of interjection\n",
    "def comment_interjection(user_comment):\n",
    "    count = Counter(tag for word,tag in user_comment)\n",
    "    return count['UH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##feature extraction\n",
    "# number of interjection\n",
    "featureddataset['interjection']  = cleaneddata.POS.apply(comment_interjection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>Numofexclaimations</th>\n",
       "      <th>Numofdots</th>\n",
       "      <th>Numofquestionmarks</th>\n",
       "      <th>SarcasticSymbol</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>NumofCapitalWords</th>\n",
       "      <th>PositiveEmojiCount</th>\n",
       "      <th>NegativeEmojiCount</th>\n",
       "      <th>parent_sentiment</th>\n",
       "      <th>interjection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I highly doubt this mostly ignored, surely uns...</td>\n",
       "      <td>The GOP has the reputation, in recent times, o...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041313</td>\n",
       "      <td>0.601616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Holy shit they are dropping an Halloween surpr...</td>\n",
       "      <td>Donald Trump Used Legally Dubious Method to Av...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Chafetz is a known liar (see PP vids) why does...</td>\n",
       "      <td>Some principles you've got there</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Kansas Number 1 in imaginary Muslim terrorists...</td>\n",
       "      <td>Kansas is probably the last state to have a te...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>wow it is totally unreasonable to assume that ...</td>\n",
       "      <td>Clinton campaign accuses FBI of 'blatant doubl...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0  I highly doubt this mostly ignored, surely uns...   \n",
       "1      0  Holy shit they are dropping an Halloween surpr...   \n",
       "2      0  Chafetz is a known liar (see PP vids) why does...   \n",
       "3      0  Kansas Number 1 in imaginary Muslim terrorists...   \n",
       "4      1  wow it is totally unreasonable to assume that ...   \n",
       "\n",
       "                                      parent_comment  Numofexclaimations  \\\n",
       "0  The GOP has the reputation, in recent times, o...                 0.0   \n",
       "1  Donald Trump Used Legally Dubious Method to Av...                 0.0   \n",
       "2                   Some principles you've got there                 0.0   \n",
       "3  Kansas is probably the last state to have a te...                 0.0   \n",
       "4  Clinton campaign accuses FBI of 'blatant doubl...                 0.0   \n",
       "\n",
       "   Numofdots  Numofquestionmarks  SarcasticSymbol  Polarity  Subjectivity  \\\n",
       "0        1.0                 0.0              0.0  0.041313      0.601616   \n",
       "1        1.0                 0.0              0.0 -0.200000      0.800000   \n",
       "2        0.0                 1.0              0.0  0.000000      0.000000   \n",
       "3        0.0                 0.0              0.0  0.200000      0.300000   \n",
       "4        0.0                 0.0              0.0 -0.100000      0.783333   \n",
       "\n",
       "   NumofCapitalWords  PositiveEmojiCount  NegativeEmojiCount  \\\n",
       "0                0.0                 0.0                 0.0   \n",
       "1                0.0                 0.0                 0.0   \n",
       "2                1.0                 0.0                 0.0   \n",
       "3                0.0                 0.0                 0.0   \n",
       "4                0.0                 0.0                 0.0   \n",
       "\n",
       "   parent_sentiment  interjection  \n",
       "0                 0             0  \n",
       "1                 1             0  \n",
       "2                 0             0  \n",
       "3                 0             0  \n",
       "4                -1             0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureddataset.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the feature column's names\n",
    "features = featureddataset.columns[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Numofexclaimations', 'Numofdots', 'Numofquestionmarks',\n",
       "       'SarcasticSymbol', 'Polarity', 'Subjectivity', 'NumofCapitalWords',\n",
       "       'PositiveEmojiCount', 'NegativeEmojiCount', 'parent_sentiment',\n",
       "       'interjection'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, test =train_test_split(featureddataset, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[features]\n",
    "y_train = train['label']\n",
    "X_test = test[features]\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest Classifier. \n",
    "clf = RandomForestClassifier(n_jobs=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=2,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Classifier to take the training features \n",
    "clf.fit(X_train, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Classifier we trained to the test data\n",
    "y_predit = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3225 1569]\n",
      " [2017 2648]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.67      0.64      4794\n",
      "           1       0.63      0.57      0.60      4665\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      9459\n",
      "   macro avg       0.62      0.62      0.62      9459\n",
      "weighted avg       0.62      0.62      0.62      9459\n",
      "\n",
      "0.6208901575219368\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_predit))  \n",
    "print(classification_report(y_test,y_predit))  \n",
    "print(accuracy_score(y_test,y_predit)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
