{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sirivamsi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Author - entire code written by Krishna Sirisha Motamarry\n",
    "# Importing Keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sirivamsi\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import Word2Vec;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training data from the csv file\n",
    "header = ['label','comment']\n",
    "data = pd.read_table('train-balanced.csv',\n",
    "                    sep='\\t', \n",
    "                   # delimiter=',', \n",
    "                    names=header,\n",
    "                    usecols=[0,1],\n",
    "                   # usecols=[0,1,9],\n",
    "                   # dtype={'label':int,'comment':str,'parent_comment':str},\n",
    "                     dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the test data from the csv file\n",
    "header = ['label','comment']\n",
    "testdata = pd.read_table('test-balanced.csv',\n",
    "                    sep='\\t', \n",
    "                   # delimiter=',', \n",
    "                    names=header,\n",
    "                    usecols=[0,1],\n",
    "                   # usecols=[0,1,9],\n",
    "                   # dtype={'label':int,'comment':str,'parent_comment':str},\n",
    "                     dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the slang dictionary that is already created\n",
    "f = open(\"Slangdictionary.txt\",\"r\")\n",
    "res1=f.read()\n",
    "f.close()\n",
    "slangdict = ast.literal_eval(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the slangs and converting the comments to sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "def comment_clean(user_comment):\n",
    "    comment_words = re.sub(r\"[^a-zA-Z0-9\\s\\']\",\"\",user_comment)         \n",
    "    comment_words=comment_words.split()\n",
    "    for word in comment_words:\n",
    "        if word.upper() in slangdict.keys():\n",
    "            user_comment = user_comment.replace(word.upper(),slangdict[word.upper()])\n",
    "        elif word in slangdict.keys():\n",
    "            user_comment = user_comment.replace(word,slangdict[word])         \n",
    "    result = text_to_word_sequence(user_comment,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True, split=' ')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  48.830397844314575\n"
     ]
    }
   ],
   "source": [
    "#clean each comment in train and test data\n",
    "import time\n",
    "start_time = time.time()\n",
    "data['comment'] = data.comment.apply(comment_clean)\n",
    "testdata['comment'] = testdata.comment.apply(comment_clean)\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatening the train and test data to create embeddings\n",
    "frames = [data,testdata]\n",
    "word2vecinput = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the data to lists\n",
    "usercomment1 = data['comment'].values.tolist()\n",
    "usercomment2 = testdata['comment'].values.tolist()\n",
    "usercomment = word2vecinput['comment'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207340\n"
     ]
    }
   ],
   "source": [
    "# Creating the word2vec embedding model\n",
    "embedding_dim = 100\n",
    "model = Word2Vec(usercomment, size=embedding_dim, window=5,workers=4, min_count=1)\n",
    "words = list(model.wv.vocab)\n",
    "print(len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model to a file\n",
    "file = \"word2vec_embedding_actualtraintestdata.txt\"\n",
    "model.wv.save_word2vec_format(file,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeninzing training data\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "tokenizer1 = Tokenizer()\n",
    "tokenizer1.fit_on_texts(usercomment1)\n",
    "seq1 = tokenizer1.texts_to_sequences(usercomment1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeninzing test data\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(usercomment2)\n",
    "seq2 = tokenizer2.texts_to_sequences(usercomment2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182577\n"
     ]
    }
   ],
   "source": [
    "# Word Index of training data\n",
    "word_index1 = tokenizer1.word_index\n",
    "print(len(word_index1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85106\n"
     ]
    }
   ],
   "source": [
    "# Word Index of test data\n",
    "word_index2 = tokenizer2.word_index\n",
    "print(len(word_index2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments are padded with the sequences\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "comment_pad1 = pad_sequences(seq1, maxlen=2000)\n",
    "comment_pad2 = pad_sequences(seq2, maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1010826,)\n",
      "(1010826, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Creating label data for training data\n",
    "labeldata1 = data['label'].values\n",
    "print(labeldata1.shape)\n",
    "print(comment_pad1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251608,)\n",
      "(251608, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Creating label data for test data\n",
    "labeldata2 = testdata['label'].values\n",
    "print(labeldata2.shape)\n",
    "print(comment_pad2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating embedding dictionary\n",
    "import os\n",
    "import numpy as np\n",
    "embeddings_dict = {}\n",
    "f = open(\"word2vec_embedding_actualtraintestdata.txt\", encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_dict[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an embedding matrix \n",
    "num_words = len(word_index1)+len(word_index2)+1\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "\n",
    "for word,i in word_index1.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "for word,i in word_index2.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning train and test data\n",
    "X_train_pad = comment_pad1\n",
    "y_train=labeldata1\n",
    "X_test_pad = comment_pad2\n",
    "y_test = labeldata2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1010826/1010826 [==============================] - 1504s 1ms/step - loss: 0.2257 - acc: 0.6232\n",
      "Epoch 2/6\n",
      "1010826/1010826 [==============================] - 1534s 2ms/step - loss: 0.2156 - acc: 0.6510\n",
      "Epoch 3/6\n",
      "1010826/1010826 [==============================] - 1558s 2ms/step - loss: 0.2104 - acc: 0.6640\n",
      "Epoch 4/6\n",
      "1010826/1010826 [==============================] - 1511s 1ms/step - loss: 0.2063 - acc: 0.6748\n",
      "Epoch 5/6\n",
      "1010826/1010826 [==============================] - 1550s 2ms/step - loss: 0.2025 - acc: 0.6834\n",
      "Epoch 6/6\n",
      "1010826/1010826 [==============================] - 1623s 2ms/step - loss: 0.1993 - acc: 0.6908\n",
      "251608/251608 [==============================] - 316s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Simple Neural Network with Word2Vec Embedding, 3 Dense layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.initializers import Constant\n",
    "from keras import layers\n",
    "from keras import callbacks\n",
    "\n",
    "cb = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, mode='auto')\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words,embedding_dim,embeddings_initializer= Constant(embedding_matrix),input_length=2000,trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "#model.add(layers.Dense(1, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "loss='mse',\n",
    "metrics=['accuracy'])\n",
    "model.fit(X_train_pad, y_train, epochs=6, batch_size=512,verbose=1)#,callbacks=[cb])\n",
    "results = model.evaluate(X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2000, 100)         26768400  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                12800064  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 39,572,689\n",
      "Trainable params: 12,804,289\n",
      "Non-trainable params: 26,768,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Model Summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.646720295251676\n"
     ]
    }
   ],
   "source": [
    "# Printing Accuracy score\n",
    "print(results[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2000, 100)         18257800  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 18,270,601\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 18,257,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Embedding, GRU\n",
    "#from keras.layers.embeddings import Embedding\n",
    "#from keras.initializers import Constant\n",
    "\n",
    "#model = Sequential()\n",
    "#embedding_layer = Embedding(num_words,embedding_dim,embeddings_initializer= Constant(embedding_matrix),input_length=2000,trainable=False)\n",
    "#model.add(embedding_layer)\n",
    "#model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "#model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
