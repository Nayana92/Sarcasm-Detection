{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from langdetect import detect  \n",
    "import ast\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, json\n",
    "from nltk import word_tokenize,pos_tag_sents,WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import nltk\n",
    "# from polyglot.detect import Detector \n",
    "# import cld2\n",
    "from sklearn.ensemble import GradientBoostingClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the emoticon look up table file\n",
    "header = ['EmoticonSymbol','SentimentScore']\n",
    "emoticon_data = pd.read_csv('EmoticonLookupTable.txt', delimiter='\\t', encoding = 'ISO-8859-1',names=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing emoticons to a dictionary\n",
    "emoji_dict = emoticon_data.groupby('EmoticonSymbol')['SentimentScore'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting acronyms and slangs from html page and creating a dictionary\n",
    "resp = requests.get(\"http://www.netlingo.com/acronyms.php\")\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "slangdict= {}\n",
    "key=\"\"\n",
    "value=\"\"\n",
    "for div in soup.findAll('div', attrs={'class':'list_box3'}):\n",
    "  for li in div.findAll('li'):\n",
    "   for a in li.findAll('a'):\n",
    "       key =a.text\n",
    "   value = li.text.split(key)[1]\n",
    "   slangdict[key]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the \"-or-\" terms in the dictionary and retaining one acronym\n",
    "for key,value in slangdict.items():\n",
    "    if \"-or-\" in value:\n",
    "       removestring = re.findall('-or-(.*)',value)\n",
    "       removestring = ''.join(removestring)\n",
    "       newvalue = value.replace(removestring,'')\n",
    "       newvalue = newvalue.replace(\"-or-\",'')\n",
    "       slangdict[key] = newvalue\n",
    "    elif \"-or\" in value:\n",
    "       removestring = re.findall('-or(.*)',value)\n",
    "       removestring = ''.join(removestring)\n",
    "       newvalue = value.replace(removestring,'')\n",
    "       newvalue = newvalue.replace(\"-or\",'')\n",
    "       slangdict[key] = newvalue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Talk to the hand'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slangdict.pop('!',None)\n",
    "slangdict.pop('*$',None)\n",
    "slangdict.pop('**//',None)\n",
    "slangdict.pop(',!!!!',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_be_replaced = []\n",
    "for keys in slangdict.keys():\n",
    "    if \" or \" in keys:\n",
    "        key_to_be_replaced.append(keys)\n",
    "\n",
    "#print(key_to_be_replaced)\n",
    "\n",
    "for keys in key_to_be_replaced:\n",
    "    getkeys = keys.split(\"or\")\n",
    "    for x in getkeys:\n",
    "        x = x.strip()\n",
    "        slangdict[x]= slangdict[keys]\n",
    "    slangdict.pop(keys,None)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Slangdictionary.txt\",'w',encoding='utf-8')\n",
    "file.write(str(slangdict))\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the training data from the csv file\n",
    "header = ['label','comment','parent_comment']\n",
    "data = pd.read_table('train-balanced.csv',\n",
    "                    sep='\\t', \n",
    "                   delimiter=',', \n",
    "                    names=header,\n",
    "                    usecols=[0,1,9],\n",
    "                    dtype={'label':int,'comment':str,'parent_comment':str},\n",
    "                    keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1010826, 3)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to clean the comments\n",
    "def comment_clean(user_comment):\n",
    "    # remove trailing \\r and \\n    \n",
    "    user_comment.rstrip('\\r\\n')\n",
    "    \n",
    "    #remove the # from hashtag\n",
    "    if '#' in user_comment:\n",
    "        hash_tag = re.search('#',user_comment)\n",
    "        if hash_tag is not None:\n",
    "            user_comment = user_comment.replace(hash_tag.group(0),' ')\n",
    "    #remove the redit tags(r/) from comment\n",
    "    if 'r/' in user_comment:\n",
    "        r_tag = re.search('r/',user_comment)\n",
    "        if r_tag is not None:\n",
    "            user_comment = user_comment.replace(r_tag.group(0),' ')\n",
    "    #remove the URL links from comments  \n",
    "    if 'HTTP' in user_comment:\n",
    "        # url of the form [link name](http://url)\n",
    "        url_link = re.search('\\[(.*)\\(HTTP(.*)\\)', user_comment)\n",
    "        if url_link is not None:\n",
    "            user_comment = user_comment.replace(url_link.group(0),' ')\n",
    "    if 'http' in user_comment:\n",
    "        # url of the form [link name](http://url)\n",
    "        url_link = re.search('\\[(.*)\\(http(.*)\\)', user_comment)\n",
    "        if url_link is not None:\n",
    "            user_comment = user_comment.replace(url_link.group(0),' ')\n",
    "        else:\n",
    "            #url of the form http:/\n",
    "            url_link = re.search('http(.*)', user_comment)\n",
    "            if url_link is not None:\n",
    "                user_comment = user_comment.replace(url_link.group(0),' ') \n",
    "                \n",
    "    # remove numbers from comments\n",
    "    user_comment_not_num = re.sub(r'\\d+', '', user_comment)   \n",
    "    \n",
    "    # Check if the comment has exactly 2 stars\n",
    "    if user_comment.count('*')==2:\n",
    "        boldwords = re.search(r\"\\*(.*?)\\*\",user_comment)\n",
    "        #print(boldwords.group(0))\n",
    "        # Check if the comments have any other text other than **\n",
    "        if boldwords.group(0) != \"**\":\n",
    "            Wordstocapitalize = re.findall(r\"\\*(.*?)\\*\",boldwords.group(0))\n",
    "            Wordstocapitalize = \"\".join( Wordstocapitalize)\n",
    "            # Replace the user comment with capitalized words\n",
    "            user_comment = user_comment.replace(boldwords.group(0),Wordstocapitalize.upper())\n",
    "    comment_words = re.sub(r\"[^a-zA-Z0-9\\s\\']\",\"\",user_comment)         \n",
    "    comment_words=comment_words.split()\n",
    "    for word in comment_words:\n",
    "        if word.upper() in slangdict.keys():\n",
    "            user_comment = user_comment.replace(word.upper(),slangdict[word.upper()])\n",
    "        elif word in slangdict.keys():\n",
    "            user_comment = user_comment.replace(word,slangdict[word]) \n",
    "        \n",
    "    # replace non english comments with empty string\n",
    "    \n",
    "    # replace non english comments with empty string\n",
    "#     try:\n",
    "#         isReliable, textBytesFound, details = cld2.detect(user_comment_not_num)\n",
    "#     except:\n",
    "#         try_text = ''.join(x for x in user_comment_not_num if x.isprintable())\n",
    "#         isReliable, textBytesFound, details = cld2.detect(try_text)\n",
    "#     cld_match = details[0][0]\n",
    "#     if not (cld_match == 'ENGLISH'):\n",
    "#         poly_match = Detector(user_comment_not_num, quiet=True).language.name\n",
    "#         if (poly_match != 'English'):\n",
    "#             user_comment = ' '               \n",
    "    return user_comment           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  78.49125504493713\n"
     ]
    }
   ],
   "source": [
    "#clean each commentnon_eng_data = []\n",
    "start_time = time.time()\n",
    "# data['comment'] = data.comment.apply(comment_clean)\n",
    "data[['comment','parent_comment']] = data[['comment','parent_comment']].applymap(comment_clean)\n",
    "# remove data with empty comments\n",
    "valid_comment = data['comment'] != ' '\n",
    "data = data[valid_comment]\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the cleaned data into a csv file\n",
    "data.to_csv('clean_data_Cldpoly.csv',\n",
    "           sep= '|',\n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the training data from the csv file\n",
    "cleaneddata = pd.read_table('clean_data_Cldpoly.csv',\n",
    "                    sep='|', \n",
    "                   # delimiter=',',\n",
    "                    usecols=[0,1,2],\n",
    "                    dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the slang dictionary that is already created\n",
    "f = open(\"Slangdictionary.txt\",\"r\")\n",
    "res1=f.read()\n",
    "f.close()\n",
    "slangdict = ast.literal_eval(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalizing the characters with *...* and Replacing the words found in slang dictionary with the full forms\n",
    "\n",
    "#def preprocessing(user_comment):\n",
    "    # Check if the comment has exactly 2 stars\n",
    "    \n",
    "#     if user_comment.count('*')==2:\n",
    "#         boldwords = re.search(r\"\\*(.*?)\\*\",user_comment)\n",
    "#         #print(boldwords.group(0))\n",
    "#         # Check if the comments have any other text other than **\n",
    "#         if boldwords.group(0) != \"**\":\n",
    "#             Wordstocapitalize = re.findall(r\"\\*(.*?)\\*\",boldwords.group(0))\n",
    "#             Wordstocapitalize = \"\".join( Wordstocapitalize)\n",
    "#             # Replace the user comment with capitalized words\n",
    "#             user_comment = user_comment.replace(boldwords.group(0),Wordstocapitalize.upper())\n",
    "#     comment_words = re.sub(r\"[^a-zA-Z0-9\\s\\']\",\"\",user_comment)         \n",
    "#     comment_words=comment_words.split()\n",
    "#     for word in comment_words:\n",
    "#         if word.upper() in slangdict.keys():\n",
    "#             user_comment = user_comment.replace(word.upper(),slangdict[word.upper()])\n",
    "#         elif word in slangdict.keys():\n",
    "#             user_comment = user_comment.replace(word,slangdict[word])            \n",
    "#     return user_comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaneddata['comment'] = cleaneddata.comment.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cleaneddata['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureextraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to get list of emojis in a comment\n",
    "def find_emoji(text):\n",
    "    return list(x for x in text.split() if x in emoji_dict.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features for each comment \n",
    "# Punctuation Features and presence of sarcastic symbol\n",
    "def allfeatures(user_comment):\n",
    "    if '!' or '.' or '?' in user_comment:\n",
    "        Numofexclaimations = user_comment.count('!')\n",
    "        Numofdots = user_comment.count('.')\n",
    "        Numofquestionmarks = user_comment.count('?')\n",
    "    else:\n",
    "        Numofexclaimations = 0\n",
    "        Numofdots = 0\n",
    "        Numofquestionmarks = 0\n",
    "    if '(!)' in user_comment:\n",
    "        SarcasticSymbol = 1\n",
    "    else:\n",
    "        SarcasticSymbol = 0\n",
    "    sentiments = TextBlob(str(user_comment)).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    numofcapitals = sum(x.isupper() for x in user_comment.split() if len(x) > 1 )\n",
    "    elist = find_emoji(user_comment)\n",
    "    pscore =0\n",
    "    nscore = 0\n",
    "    for item in elist:\n",
    "        if (emoji_dict[item][0] == 1):\n",
    "            pscore += 1\n",
    "        elif (emoji_dict[item][0] == -1):\n",
    "            nscore += 1\n",
    "    return Numofexclaimations,Numofdots,Numofquestionmarks,SarcasticSymbol,polarity,subjectivity,numofcapitals,pscore,nscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  1182.7921845912933\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "featureddataset = featureextraction(cleaneddata, 'comment', allfeatures, ['Numofexclaimations', 'Numofdots','Numofquestionmarks','SarcasticSymbol','Polarity', 'Subjectivity','NumofCapitalWords','PositiveEmojiCount','NegativeEmojiCount'])\n",
    "end_time = time.time() \n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start_time = time.time()\n",
    "featureddataset = featureextraction(cleaneddata, 'comment', allfeatures, ['Numofexclaimations', 'Numofdots','Numofquestionmarks','SarcasticSymbol','Polarity', 'Subjectivity','NumofCapitalWords','PositiveEmojiCount','NegativeEmojiCount'])\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  587.7420217990875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "txt = cleaneddata['comment'].tolist()\n",
    "#POS tagging for all the tokens in the sentence\n",
    "tagged_texts = pos_tag_sents(map(word_tokenize, txt))\n",
    "end_time = time.time()\n",
    "cleaneddata['POS'] = tagged_texts\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#helper function to collect number of interjection\n",
    "def comment_interjection(user_comment):\n",
    "    count = Counter(tag for word,tag in user_comment)\n",
    "    return count['UH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##feature extraction\n",
    "# number of interjection\n",
    "featureddataset['interjection']  = cleaneddata.POS.apply(comment_interjection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_sentiment(comment):\n",
    "    sentiments = TextBlob(str(comment)).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    if polarity >= 0.1:\n",
    "        return 1\n",
    "    elif polarity < -0.1:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  539.8422358036041\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "featureddataset['parent_sentiment'] = cleaneddata.parent_comment.apply(get_parent_sentiment)\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the Glove embedding into memory\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Tokenize the comments\n",
    "Word_tokenizer = Tokenizer()\n",
    "Word_tokenizer.fit_on_texts(featureddataset['comment'])\n",
    "# Word_tokenizer.num_words = 100000\n",
    "vocab_size = len(Word_tokenizer.word_index) + 1\n",
    "#encode the train tokens to sequence\n",
    "sequences = Word_tokenizer.texts_to_sequences(featureddataset['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 179 3945    5  179  497]]\n",
      "[ 179 3945    5  179  497]\n",
      "[ 179 3945    5  179  497]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "type(sequences[0])\n",
    "myarray = np.asarray(sequences[0])\n",
    "x = np.reshape(sequences[0], (len(sequences[0]),1)).T\n",
    "print(x)\n",
    "print(myarray)\n",
    "print(myarray.T)\n",
    "cos = 179*5 / (np.sqrt(179*179) * np.sqrt(5*5))\n",
    "print(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_words = 100000\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in Word_tokenizer.word_index.items():\n",
    "#     if i >= num_words:\n",
    "#         continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182821, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "embedding_square ={}\n",
    "keys = range(vocab_size)\n",
    "for i in keys:\n",
    "    embedding = embedding_matrix[i]\n",
    "    sum_square = 0\n",
    "    for j in range(len(embedding)):\n",
    "        values = embedding[j]\n",
    "        sum_square += values*values\n",
    "    embedding_square[i] = math.sqrt(sum_square)\n",
    "# print(embedding_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182821"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"embedding_square.txt\",'w',encoding='utf-8')\n",
    "file.write(str(embedding_square))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the cleaned data with features into a csv file\n",
    "featureddataset.to_csv('clean_data_with features.csv',\n",
    "           sep= '|',\n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.358939569778979"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(embedding_matrix[179] ,embeddings_index.get('nice'))\n",
    "embedding_square[179]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cosine_similarity(word1,word2,v1,v2):\n",
    "#     \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxy = 0\n",
    "#     print(word1,word2)\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "#         sumxx += x*x\n",
    "#         sumyy += y*y\n",
    "        sumxy += x*y\n",
    "#     print(sumxy)\n",
    "#     print(embedding_square[word1]*embedding_square[word1])\n",
    "#     cosine = sumxy/(embedding_square[word1]*embedding_square[word2])\n",
    "#     print(cosine)\n",
    "#     return cosine\n",
    "    return sumxy/(embedding_square[word1]*embedding_square[word2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "0\n",
      "15\n",
      "0\n",
      "(1, 4)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "x = np.matrix([ 0,  1,  2,  15])\n",
    "# x = matrix([[ 0,  1,  2,  15],\n",
    "#         [ 4,  12,  6,  7],\n",
    "#         [ 8,  9, 10, 11]])\n",
    "\n",
    "# print(x.max(x.max(0)))\n",
    "# print(x.max(0))\n",
    "y = x.max(0)\n",
    "print(y.max())\n",
    "print(y.min())\n",
    "# print(x.min(0))\n",
    "z = x.min(0)\n",
    "print(z.max())\n",
    "print(z.min())\n",
    "# print(x.max(1))\n",
    "a=x.shape\n",
    "print(a)\n",
    "print(x.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(comment_token):\n",
    "    token_array = np.matrix(comment_token)\n",
    "    comment_len = token_array.shape[1]\n",
    "#     print(comment_token[0])\n",
    "#     print(comment_len)\n",
    "    most_similar = least_similar = most_dissimilar = least_dissimilar = 0\n",
    "    if comment_len > 0:\n",
    "        mat = np.zeros(shape=(comment_len,comment_len))\n",
    "        for i in range(0,comment_len):\n",
    "            for j in range(i+1,comment_len):\n",
    "                a = comment_token[i]\n",
    "                b = comment_token[j]\n",
    "                mat[i][j] = cosine_similarity(a,b,embedding_matrix[a],embedding_matrix[b].T)\n",
    "                mat[j][i] = mat[i][j]\n",
    "#         print(mat)\n",
    "    #get the most similar\n",
    "        similar_mat = mat.max(0)\n",
    "        if()\n",
    "        most_similar = similar_mat.max()\n",
    "        least_similar = similar_mat.min()\n",
    "        dissimilar_mat = mat.min(0)\n",
    "        most_dissimilar = dissimilar_mat.max()\n",
    "        least_dissimilar = dissimilar_mat.min()\n",
    "        \n",
    "    return most_similar, least_similar, most_dissimilar, least_dissimilar            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.718233312542907\n",
      "28.71823331254291\n",
      "0.9999999999999999\n",
      "[[0. 1.]\n",
      " [1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9999999999999999, 0.9999999999999999, 0.0, 0.0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_similarity([179,179])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "5\n",
      "179 3945\n",
      "2.660808595734904\n",
      "179 5\n",
      "11.38494467833491\n",
      "179 179\n",
      "28.718233312542907\n",
      "179 497\n",
      "12.338481814359477\n",
      "3945 5\n",
      "-0.07627495684197177\n",
      "3945 179\n",
      "2.660808595734904\n",
      "3945 497\n",
      "4.106126702458106\n",
      "5 179\n",
      "11.38494467833491\n",
      "5 497\n",
      "19.017276923175036\n",
      "179 497\n",
      "12.338481814359477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9999999999999999, 0.16893828121130872, 0.0, -0.0028069642283230986)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "calculate_similarity([179, 3945, 5, 179, 497])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.18554001,  0.047152  ,  0.34867001, -0.23114   , -0.26082999,\n",
       "        0.63107997,  0.55686998,  0.61622   , -0.15436999, -0.38381001,\n",
       "        0.12445   , -0.26999   , -0.29196   ,  0.1125    ,  0.36035001,\n",
       "        0.70688999, -0.33891001, -0.26949999,  0.17481001,  0.97048002,\n",
       "        0.23014   ,  0.63168001, -0.24542999, -0.72890002,  0.32517001,\n",
       "       -0.21118   , -0.80353999, -0.59863001, -0.10182   , -0.87826002,\n",
       "       -0.80162001,  0.20998999,  0.64598   , -0.38238999,  0.64512002,\n",
       "        0.73045999, -0.19881   ,  0.35716999,  0.19135   , -0.43686   ,\n",
       "        0.75955999, -0.66430998,  0.34509   , -1.03830004, -0.50490999,\n",
       "        0.19976   , -0.041208  ,  0.16952001,  0.17821001, -0.84249002,\n",
       "        0.21991999, -0.16474   , -0.24669001,  0.34117001, -0.59713   ,\n",
       "       -2.3434    ,  0.31483999,  0.69668001,  0.53368002, -0.62857002,\n",
       "       -0.197     ,  0.52240998, -1.59029996, -0.16474999,  0.62553   ,\n",
       "       -0.094116  ,  0.0070705 ,  0.22617   , -0.45697999, -0.53267998,\n",
       "        0.11573   , -0.19052   ,  0.28086001, -0.55909997,  0.35291001,\n",
       "        0.42552   ,  0.47334   , -0.41046   ,  0.47275001,  0.22753   ,\n",
       "       -0.097073  , -0.11809   , -0.25613001, -0.10689   , -1.27880001,\n",
       "       -0.085504  ,  0.20205   ,  0.23338   , -0.52938998, -0.49491999,\n",
       "        0.15289   , -0.31323999,  0.41248   , -0.10425   , -0.65245998,\n",
       "        0.48348001, -0.54843998,  0.095473  , -0.1142    ,  0.32743001])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[179]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureddataset['embedding'] = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>Numofexclaimations</th>\n",
       "      <th>Numofdots</th>\n",
       "      <th>Numofquestionmarks</th>\n",
       "      <th>SarcasticSymbol</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>NumofCapitalWords</th>\n",
       "      <th>PositiveEmojiCount</th>\n",
       "      <th>NegativeEmojiCount</th>\n",
       "      <th>interjection</th>\n",
       "      <th>parent_sentiment</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Nice Crib  and Nice Hand.</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[179, 3945, 5, 179, 497]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[6, 45, 61, 964, 694, 171, 255, 964, 694, 55, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>They're favored to win.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[20, 106, 20010, 1940, 313, 13, 207, 43164, 34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193182</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[14, 1039, 132, 332, 946, 8, 1, 146, 2324, 476...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[4, 94, 134, 47, 8, 89, 3439]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0                          Nice Crib  and Nice Hand.   \n",
       "1      0  You do know west teams play against west teams...   \n",
       "2      0  They were underdogs earlier today, but since G...   \n",
       "3      0  This meme isn't funny none of the \"new york ni...   \n",
       "4      0                    I could use one of those tools.   \n",
       "\n",
       "                                      parent_comment  Numofexclaimations  \\\n",
       "0  Yeah, I get that argument. At this point, I'd ...                 0.0   \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...                 0.0   \n",
       "2                            They're favored to win.                 0.0   \n",
       "3                         deadass don't kill my buzz                 0.0   \n",
       "4  Yep can confirm I saw the tool they use for th...                 0.0   \n",
       "\n",
       "   Numofdots  Numofquestionmarks  SarcasticSymbol  Polarity  Subjectivity  \\\n",
       "0        1.0                 0.0              0.0  0.600000      1.000000   \n",
       "1        0.0                 1.0              0.0  0.392857      0.517857   \n",
       "2        0.0                 0.0              0.0  0.000000      0.500000   \n",
       "3        1.0                 0.0              0.0  0.193182      0.727273   \n",
       "4        1.0                 0.0              0.0  0.000000      0.000000   \n",
       "\n",
       "   NumofCapitalWords  PositiveEmojiCount  NegativeEmojiCount  interjection  \\\n",
       "0                0.0                 0.0                 0.0             0   \n",
       "1                0.0                 0.0                 0.0             0   \n",
       "2                0.0                 0.0                 0.0             0   \n",
       "3                0.0                 0.0                 0.0             0   \n",
       "4                0.0                 0.0                 0.0             0   \n",
       "\n",
       "   parent_sentiment                                          embedding  \n",
       "0                 1                           [179, 3945, 5, 179, 497]  \n",
       "1                 1  [6, 45, 61, 964, 694, 171, 255, 964, 694, 55, ...  \n",
       "2                 1  [20, 106, 20010, 1940, 313, 13, 207, 43164, 34...  \n",
       "3                 0  [14, 1039, 132, 332, 946, 8, 1, 146, 2324, 476...  \n",
       "4                 0                      [4, 94, 134, 47, 8, 89, 3439]  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureddataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  7557.038921833038\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "embedded_dataset = featureextraction(featureddataset, 'embedding', calculate_similarity, ['most_similar','least_similar','most_dissimilar','least_dissimilar'])\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {'most_similar': 0, 'least_similar': 0, 'most_dissimilar': 0, 'least_dissimilar': 0}\n",
    "embedded_dataset = embedded_dataset.fillna(value=values)\n",
    "embedded_dataset = embedded_dataset.drop(columns=['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>Numofexclaimations</th>\n",
       "      <th>Numofdots</th>\n",
       "      <th>Numofquestionmarks</th>\n",
       "      <th>SarcasticSymbol</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>NumofCapitalWords</th>\n",
       "      <th>PositiveEmojiCount</th>\n",
       "      <th>NegativeEmojiCount</th>\n",
       "      <th>interjection</th>\n",
       "      <th>parent_sentiment</th>\n",
       "      <th>most_similar</th>\n",
       "      <th>least_similar</th>\n",
       "      <th>most_dissimilar</th>\n",
       "      <th>least_dissimilar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Nice Crib  and Nice Hand.</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.168938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>They're favored to win.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193182</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.777721</td>\n",
       "      <td>0.634564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      0                          Nice Crib  and Nice Hand.   \n",
       "1      0  You do know west teams play against west teams...   \n",
       "2      0  They were underdogs earlier today, but since G...   \n",
       "3      0  This meme isn't funny none of the \"new york ni...   \n",
       "4      0                    I could use one of those tools.   \n",
       "\n",
       "                                      parent_comment  Numofexclaimations  \\\n",
       "0  Yeah, I get that argument. At this point, I'd ...                 0.0   \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...                 0.0   \n",
       "2                            They're favored to win.                 0.0   \n",
       "3                         deadass don't kill my buzz                 0.0   \n",
       "4  Yep can confirm I saw the tool they use for th...                 0.0   \n",
       "\n",
       "   Numofdots  Numofquestionmarks  SarcasticSymbol  Polarity  Subjectivity  \\\n",
       "0        1.0                 0.0              0.0  0.600000      1.000000   \n",
       "1        0.0                 1.0              0.0  0.392857      0.517857   \n",
       "2        0.0                 0.0              0.0  0.000000      0.500000   \n",
       "3        1.0                 0.0              0.0  0.193182      0.727273   \n",
       "4        1.0                 0.0              0.0  0.000000      0.000000   \n",
       "\n",
       "   NumofCapitalWords  PositiveEmojiCount  NegativeEmojiCount  interjection  \\\n",
       "0                0.0                 0.0                 0.0             0   \n",
       "1                0.0                 0.0                 0.0             0   \n",
       "2                0.0                 0.0                 0.0             0   \n",
       "3                0.0                 0.0                 0.0             0   \n",
       "4                0.0                 0.0                 0.0             0   \n",
       "\n",
       "   parent_sentiment  most_similar  least_similar  most_dissimilar  \\\n",
       "0                 1      1.000000       0.168938              0.0   \n",
       "1                 1      1.000000       0.583701              0.0   \n",
       "2                 1      0.000000       0.000000              0.0   \n",
       "3                 0      0.000000       0.000000              0.0   \n",
       "4                 0      0.777721       0.634564              0.0   \n",
       "\n",
       "   least_dissimilar  \n",
       "0         -0.002807  \n",
       "1          0.000000  \n",
       "2          0.000000  \n",
       "3          0.000000  \n",
       "4          0.000000  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[354]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import dot\n",
    "# from numpy.linalg import norm\n",
    "\n",
    "# cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedded_dataset.drop(columns=['embedding'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "# training_data, test_data = train_test_split(embedded_dataset, test_size=0.20)\n",
    "training_data, test_data, y_train, y_test = train_test_split(embedded_dataset, embedded_dataset['label'], \n",
    "                                                    test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808624, 18)\n"
     ]
    }
   ],
   "source": [
    "# embedded_dataset.drop(columns=['embedding'])\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtrain = pd.DataFrame(training_data.iloc[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808624, 15)\n"
     ]
    }
   ],
   "source": [
    "print(newtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetlabel = training_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808624,)\n"
     ]
    }
   ],
   "source": [
    "print(targetlabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  44.04390478134155\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5, max_depth=1)\n",
    "clf.fit(newtrain,targetlabel)\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_new = test_data.iloc[:, 3:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202157, 15)\n"
     ]
    }
   ],
   "source": [
    "print(test_data_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_target = test_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202157,)\n"
     ]
    }
   ],
   "source": [
    "print(test_data_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(test_data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[71179 29832]\n",
      " [52228 48918]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.70      0.63    101011\n",
      "           1       0.62      0.48      0.54    101146\n",
      "\n",
      "   micro avg       0.59      0.59      0.59    202157\n",
      "   macro avg       0.60      0.59      0.59    202157\n",
      "weighted avg       0.60      0.59      0.59    202157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(test_data_target, predictions))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(test_data_target, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5940778701702142\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_data_target, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "# for learning_rate in learning_rates:\n",
    "#     gb = GradientBoostingClassifier(n_estimators=20, learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0)\n",
    "#     gb.fit(newtrain,targetlabel)\n",
    "#     predictions = clf.predict(test_data_new)\n",
    "#     print(\"Confusion Matrix\")\n",
    "#     print(confusion_matrix(test_data_target, predictions))\n",
    "#     print()\n",
    "#     print(\"Classification Report\")\n",
    "#     print(classification_report(test_data_target, predictions))\n",
    "#     print()\n",
    "#     print(accuracy_score(test_data_target, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the feature column's names\n",
    "features = embedded_dataset.columns[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data[features]\n",
    "y_train = training_data['label']\n",
    "X_test = test_data[features]\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create a random forest Classifier. \n",
    "clf = RandomForestClassifier(n_jobs=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  14.977846622467041\n"
     ]
    }
   ],
   "source": [
    "# Train the Classifier to take the training features \n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train )\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Classifier we trained to the test data\n",
    "y_predit = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[69374 31637]\n",
      " [51588 49558]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.69      0.63    101011\n",
      "           1       0.61      0.49      0.54    101146\n",
      "\n",
      "   micro avg       0.59      0.59      0.59    202157\n",
      "   macro avg       0.59      0.59      0.58    202157\n",
      "weighted avg       0.59      0.59      0.58    202157\n",
      "\n",
      "0.588315022482526\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_predit))  \n",
    "print(classification_report(y_test,y_predit))  \n",
    "print(accuracy_score(y_test,y_predit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "start_time = time.time()\n",
    "# linear kernel\n",
    "svclassifier = SVC(kernel='linear',C=1,gamma=1)  \n",
    "svclassifier.fit(X_train, y_train) \n",
    "\n",
    "y_pred = svclassifier.predict(X_test)  \n",
    "\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guassian kernel\n",
    "start_time = time.time()\n",
    "svclassifier = SVC(kernel='rbf',C=1,gamma=1)  \n",
    "svclassifier.fit(X_train, y_train) \n",
    "\n",
    "y_pred = svclassifier.predict(X_test)  \n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial kernel with degree 3\n",
    "start_time = time.time()\n",
    "svclassifier = SVC(kernel='poly', degree=3) \n",
    "svclassifier.fit(X_train, y_train) \n",
    "\n",
    "y_pred = svclassifier.predict(X_test) \n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
